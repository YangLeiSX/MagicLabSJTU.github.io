---
# 论文完整标题
title: 'Personatalk: Perserving Personalized Dynamic Speech Style in Talking Face Generation'

# 论文作者，此处仅需填写本实验室成员（包括王老师）即可，使用中文姓名
authors:
  - 陆千禧
  - 何怡
  - 王士林

# 论文发表时间，年-月-日，大致即可
date: '2024-10-01'

# 论文类型， 可选：conference, journal
publication_types: ['conference']

# 会议/期刊名称及缩写
publication: In *Proceedings of IEEE International Conference on Image Processing 2024*
publication_short: In *ICIP 2024*

# 论文摘要，不要有换行
abstract: Recent research highlights the robust authentication performance of lip-based speaker authentication methods, claiming their effectiveness against deepfake attacks. However, we contend that the attributed success is rooted in the inadequacy of existing talking face generation methods to preserve the dynamic speech style of speakers. To address this, we propose PersonaTalk, a speaker-specific method utilizing the speaker’s video data to enhance the fidelity of personalized dynamic speech styles in generated videos. Our approach introduces a visual context block to integrate lip motion information into the audio features. Additionally, to enhance reading intelligibility in dubbed videos, a cross dubbing phase is incorporated during training. Experiments on the GRID dataset show the superiority of PersonaTalk over existing SOTA methods. These findings advocate for heightened defense requirements in existing lip-based speaker authentication methods.

# 后续内容无需修改
url_pdf: ''
---